---
title: "ISLR Chapter 4 - conceptual"
output: html_document
---

## Exercise 1

**Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.**

We want to show that this expression for $p(X)$:

$$
p(X)=\frac{e^{\beta_0 + \beta_1X}}{1+e^{\beta_0 + \beta_1X}}
$$

is equivalent to this expression for the odds:

$$
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1X}
$$

From the first expression we can write:

$$1-p(X)=1-\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0 + \beta_1X}}$$

Which simplifies to:

$$
1-p(X)=\frac{1}{1+e^{\beta_0 + \beta_1X}}
$$

Hence:

$$\frac{p(X)}{1-p(X)}=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\cdot\frac{1+e^{\beta_0+\beta_1X}}{1}=e^{\beta_0+\beta_1X}$$

## Exercise 2

**It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $N(μ_k,σ2)$ distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized.**

We can show this by applying only monotonic transformations to the density function in order to arrive at the discriminant function. In other words, can we apply transformations that do not alter the order of values given by each function to get from one to the other?

The posterior probability that an observation of $x$ is in class $k$:

$$
p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum^K_{l=1}\pi_l\frac{1}{\sqrt{2\pi\sigma}}\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}
$$

We are maximising over the possible $K$ classes; that is to say, for a fixed value of $x$, we want to find the class $k$ for which $p_k(x)$ is highest. Therefore, all terms involving anything other than the parameter $k$ can be treated as constants. 

Firstly, we can multiply $p_k(x)$ by the denominator. This is a summation across all classes so is a constant, and hence multiplying by it is a monotonic transformation. Similarly, we can multiply by $\sqrt{2\pi\sigma}$, which is a constant and hence also a monotonic transformation (since we assume all classes share a common variance). The $\sqrt{2\pi\sigma}$ terms cancel on the left hand side, giving the following:

$$
p_k(x)\cdot \sum^K_{l=1}\pi_l\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)=\pi_k\exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)
$$

Next, we take logs:

$$
\log(p_k(x)\cdot \sum^K_{l=1}\pi_l\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2))=\log(\pi_k\exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2))
$$
$$
=\log(\pi_k)-\frac{1}{2\sigma^2}(x-\mu_k)^2
$$
$$
=\log(\pi_k)-\frac{1}{2\sigma^2}(x^2-2x\mu_k+\mu_k^2)
$$
$$
=\log(\pi_k)-\frac{x^2}{2\sigma^2}+x\cdot\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}
$$

Again, because we are maximising for a fixed value of $x$ and all classes have a common variance $\sigma^2$, $\frac{x^2}{2\sigma^2}$ is a constant so we can add it to this expression:

$$
\log(p_k(x)\cdot \sum^K_{l=1}\pi_l\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2))+\frac{x^2}{2\sigma^2}=x\cdot\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log\pi_k
$$

Let:
$$
\delta_k(x)=\log(p_k(x)\cdot \sum^K_{l=1}\pi_l\exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2))+\frac{x^2}{2\sigma^2} 
$$

So we can write the following:
$$
\delta_k(x)=x\cdot\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2}+\log\pi_k
$$

The discriminant function is hence a monotonic transformation of the probability $p_k(x)$, and finding the class $k$ where the former is maximised is the same as finding this class for the maximum of the latter. 

## Exercise 3

**This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature.**

**Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, $X ∼ N(μ_k,σ_k2)$. Recall that the density function for the one-dimensional normal distribution is given in (4.11). Prove that in this case, the Bayes’ classifier is not linear. Argue that it is in fact quadratic.**

We can plug the distributions $X ∼ N(μ_k,σ_k2)$ for each class $k$ (one of $K$ classes) into Bayes' theorem to get:

$$
p_k(x)=\frac{\pi_k\frac{1}{\sqrt{2\pi\sigma_k}}\exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)}{\sum^K_{l=1}\pi_l\frac{1}{\sqrt{2\pi\sigma_l}}\exp(-\frac{1}{2\sigma_l^2}(x-\mu_l)^2)}
$$

Similar to the case with common variance between classes, the denominator is a constant as it is a sum across all classes in $K$, so multiplying by this term is a monotonic transformation of  $p_k(x)$. We can then take logs to get rid of the $\exp$ term. The discriminant function where we have class specific variances $\sigma_k^2$ for each class $k$ is therefore given by the following: 

$$
\delta_k(x)=\log(\frac{\pi_k}{\sqrt{2\pi\sigma_k}})-\frac{x^2}{2\sigma_k^2}+x\cdot\frac{\mu_k}{\sigma_k^2}-\frac{\mu_k^2}{2\sigma_k^2}
$$

Note that the $\frac{1}{\sqrt{2\pi\sigma_k}}$ and $-\frac{x^2}{2\sigma_k^2}$ terms are no longer constant for a given value of $x$ across each class in $K$ on account of the class specific variance so we must keep them in the discriminant function. 

The presence of an $x^2$ term suggests the discriminant function is quadratic and so does not vary linearly with $x$.

To illustrate this, consider two given classes $a$ and $b$ and the decision boundary between them. An observation of $x$ will be classified to class $a$ where:

$$
\delta_a(x)>\delta_b(x)
$$
$$
\log(\frac{\pi_a}{\sqrt{2\pi\sigma_a}})-\frac{x^2}{2\sigma_a^2}+x\cdot\frac{\mu_a}{\sigma_a^2}-\frac{\mu_a^2}{2\sigma_a^2}>\log(\frac{\pi_b}{\sqrt{2\pi\sigma_b}})-\frac{x^2}{2\sigma_b^2}+x\cdot\frac{\mu_b}{\sigma_b^2}-\frac{\mu_b^2}{2\sigma_b^2}
$$

We can rewrite this in the familiar $ax^2+bx+c$ form for quadratic expressions:

$$
x^2(\frac{1}{2\sigma_b^2}-\frac{1}{2\sigma_a^2})+x(\frac{\mu_a}{\sigma_a^2}-\frac{\mu_b}{\sigma_b^2})+\frac{\mu_b^2}{2\sigma_b^2}-\frac{\mu_a^2}{2\sigma_a^2}+\log(\frac{\pi_a}{\sqrt{2\pi\sigma_a}})-\log(\frac{\pi_b}{\sqrt{2\pi\sigma_b}})>0
$$

The classifier is quadratic in $x$; where this function is greater than zero for a value of $x$, the observation is classified to class $a$, otherwise, it is classified to class $b$.

## Exercise 4

**When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse.**

**Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $[0, 1]$. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the prediction?**

As $X$ is uniformly distributed, a range that covers 10% of the range of $X$ will correspond to 10% of observations. Hence, we would use 10% of the available observations for each prediction. 

**Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_1$ and $X_2$. We assume that $(X_1 , X_2)$ are uniformly distributed on $[0, 1]\times[0, 1]$. We wish to predict a test observation’s response using only observations that are within 10% of the range of X$_1$ and within 10% of the range of $X_2$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in the range $[0.55, 0.65]$ for $X_1$ and in the range $[0.3, 0.4]$ for $X_2$. On average, what fraction of the available observations will we use to make the prediction?**

Each set of observations within the 10% ranges will lie within a square in the 2-dimensions $X_1$ and $X_2$, so the fraction of available observations would be the square of the previous answer, 1%. 

**Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10 % of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?**

Following the same logic as the 2 dimensional case, this will be $0.1^{100}$.

**Using your answers to parts (a)–(c), argue that a drawback of KNN when $p$ is large is that there are very few training observations “near” any given test observation.**

As $p$ grows larger, the number of observations near any given point exponentially shrinks - as dimensions increase, the points get further away from one another as they vary in an increasing number of dimensions. Hence if we used KNN, we would be using neighbouring observations that are very far from a test observation and so are unlikely to yield good predictions.

**Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p$ = 1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.**

For $p=1$, the hypercube is one-dimensional and hence a straight line. The line that contains 10% of the observations would just be 10% of the data, hence it would be of length 0.1.

For $p=2$, we would have a square. A square would need to encompass an area of 0.1 to cover 10% of the observations, which means each side would be of length $\sqrt{0.1}$, which is:
```{r}
sqrt(0.1)
```

For $p=100$, following the same logic, the 100-dimensional hypercube would need to have sides of length $\sqrt[100]{0.1}$, which is almost 1. This means the these observations will be distributed across almost the entire range of each predictor.


## Exercise 5

**We now examine the differences between LDA and QDA.**

**If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?**

With a linear Bayes decision boundary, we would still expect QDA to perform better on the training set than the test set as it would overfit the data given it is more flexible (and hence has lower bias). On the test set, LDA would outperform QDA as it is closer to the Bayes classifier.

**If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?**

If the Bayes decision boundary is non-linear, we would expect QDA to outperform LDA on both the training and test sets. 

**In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?**

In general, we would expect the test prediction accuracy of QDA relative to LDA to increase as n increases. With a high n, QDA is less likely to overfit - in other words, QDA's flexibility would be less likely to produce estimates with high variance when trained on a larger set of observations, while LDA will suffer from high bias. 

**True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.**

False - QDA will simply overfit our training data and so would be unlikely to adequately model the underlying linear decision boundary. 

## Exercise 6

**Suppose we collect data for a group of students in a statistics class with variables $X_1$ = hours studied, $X_2$ = undergrad GPA, and $Y$ = receive an A. We fit a logistic regression and produce estimated coefficients, $β_0$ = −6, $β_1$ = 0.05, $β_2$ = 1**

**Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.**

```{r}
exp(-6 + 0.05*40 + 1*3.5)/(1 + exp(-6 + 0.05*40 + 1*3.5))
```

**How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?**

Substituting in 50% for the chance of getting an A and a 3.5 GPA, and rearranging for $X_1$:
$$
0.5=\frac{e^{-6+0.05\cdot{X_1}+3.5}}{1+e^{-6+0.05\cdot{X_1}+3.5}}
$$
$$
0.5+0.5(e^{-6+0.05\cdot{X_1}+3.5})=e^{-6+0.05\cdot{X_1}+3.5}
$$
$$
0.5=0.5(e^{-6+0.05\cdot{X_1}+3.5})
$$
$$
1=e^{-6+0.05\cdot{X_1}+3.5}
$$
$$
\log(1)=-6+0.05\cdot{X_1}+3.5
$$
$$
X_1=\frac{\log(1)+6-3.5}{0.05}
$$

```{r}
(log(1) + 6 - 3.5)/0.05
```

The student would have to work 50 hours per week .

## Exercise 7

**Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on $X$, last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\bar{X}$ = 10, while the mean for those that didn’t was $\bar{X}$ = 0. In addition, the variance of $X$ for these two sets of companies was $\sigma^2$ = 36. Finally, 80% of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X$ = 4 last year.**

Substituting the values given in the question into the density function for a normal random variable, the normal density functions for each class are:
$$
f_{Yes}(x)=\frac{1}{72\pi}e^{-(x-10)^2/72}
$$
$$
f_{No}(x)=\frac{1}{72\pi}e^{-x^2/72}
$$

These functions take the following values at $X$ = 4.
```{r}
y_density <- exp(-(4-10)^2/72)/(72*pi)
print(y_density)
n_density <- exp(-4^2/72)/(72*pi)
print(n_density)
```

Bayes theorem states that:
$$
\Pr(Y=K|X=x)=\frac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}
$$
We want the probability that the company issues a dividend given that $X$ = 4. Using Bayes theorem:
$$
\Pr(Y=Yes|X=4)=\frac{0.8\cdot{f_{Yes}(4)}}{0.2\cdot{f_{No}(4)}+0.8\cdot{f_{Yes}(4)}}
$$

The probability that the company pays a dividend is about 0.75.
```{r}
pr_yes <- 0.8*y_density/(0.2*n_density + 0.8*y_density)
pr_yes
```

## Exercise 8

**Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30 % on the test data. Next we use 1-nearest neighbours (i.e. $K$ = 1) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?**

If all training observations are available to pick as a nearest neighbour when training KNN, then training error will be 0%. The point we are predicting can have itself as its nearest neighbour because it would be its own closest point in the training set and so will always be classified correctly. Test error, which we would determine by picking one of the n training observations as the nearest neighbour, would therefore be 36%. We should hence pick logisitic regression, which has a lower test error rate.

## Exercise 9

**This problem has to do with odds.**

**On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?**

We can write the odds as follows, where $p$ is the probability of default:

$$
\frac{p}{1-p}=0.37
$$

Solving for $p$:

$$
p=0.37-0.37p\\
1.37p=0.37\\
p=\frac{0.37}{1.37}
$$
```{r}
0.37/1.37
```
 27% of people with an odds of 0.37 of default will default. 
 
**Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?** 

```{r}
0.16/(1-0.16)
```
The odds of this person defaulting are 0.19.
