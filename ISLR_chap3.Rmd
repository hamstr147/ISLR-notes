---
title: "ISLR Chapter 3 exercises"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Exercise 1

Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.
The null hypotheses to which the 

# Exercise 4

We would expect the training RSS to be lower in the model with the cubic term than that with the single predictor even if the actual relationship is linear. Simply adding extra variables will result in a lower training RSS as the model would overfit the training data. 
In contrast, the test RSS will be lower for the single predictor model because this more closely fits the actual relationship - the increased bias in the simple linear regression would be more than offset by the decrease in variance. 
If the actual relationship is non-linear, we would expect the training RSS of the cubic term to be less than the training RSS of the simple model, regardless of how non-linear it is for the same reason outlined above. 
However, for the test RSS we do not have enough information to say either way, as if the relationship is only slightly non-linear, the linear model may be a good approximation of the true function. The result due to the bias-variance trade off here is uncertain.

# Exercise 5

For a linear regression without an intercept, the ith fitted value takes the form:
$$\hat{y}_i = \hat{\beta}_1x_i$$
where 
$$\hat{\beta}_1 = \frac{\sum_{i=1}^nx_iy_i}{\sum_{i=1}^nx_i^2}$$
Show that we can write:
$$\hat{y}_i = \sum_{i'=1}^na_{i'}y_{i'}$$

Substituting in $\hat{\beta}_1$:
$$\hat{y}_i = \frac{\sum_{i'=1}^nx_{i'}y_{i'}}{\sum_{i''=1}^nx_{i''}^2}\cdot{x_i} \\
= \sum_{i'=1}^n\frac{x_ix_{i'}}{\sum_{i''=1}^nx_{i''}^2}\cdot{y_{i'}}$$

Hence $a_{i'}$ is:
$$a_{i'} = \frac{x_ix_{i'}}{\sum_{i''=1}^nx_{i''}^2}$$

# Exercise 6

The intercept in the simple linear regression case is:
$$\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$$
The regression line can be written as:
$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$$

As the a simple regression is linear, there must be a point on the regression line that passes through $\bar{x}$. It is clear from the intercept $\beta_0$ that where $x = \bar{x}$ we have the following:
$$\hat{y} = \bar{y} - \hat{\beta_1\bar{x}} + \hat{\beta_1}\bar{x} \\
= \bar{y}$$

Hence, where the regression line passes through $\bar{x}$, it must at this point also pass through $\bar{y}$, and hence the regression line must pass through $(\bar{x},\bar{y})$.

# Exercise 7

Show that in the simple linear regression case, the squared correlation equals $R^2$. You may assume that the mean of both X and Y are 0. 

$R^2$ is defined as:
$$R^2=1-\frac{RSS}{TSS}$$

Under the assumption of zero means for X and Y, we know the following:
$$\beta_0 = 0$$
$$\beta_1 = \frac{\sum_{i=1}^n{x_iy_i}}{\sum_{i=1}^n{x_i^2}}$$
$$TSS = \sum_{i=1^n}{y_i^2}$$
$$RSS = \sum_{i=1}^n{(y_i - \beta_1x_i)^2} = \sum_{i=1}^n{(y_i^2-2\beta_1x_iy_i+\beta_1^2x_i^2)}$$
Substituting for $\beta_1$:
$$RSS = \sum_{i=1}^n{(y_i^2-2\beta_1x_iy_i+\beta_1^2x_i^2)} \\
= \sum_{i=1}^n{y_i^2} - 2\frac{(\sum_{i=1}^nx_iy_i)^2}{\sum_{i=1}^nx_i^2} + \sum_{i=1}^n\beta_1^2x_i^2 \\
= \sum_{i=1}^n{y_i^2} - 2\beta_1^2\sum_{i=1}^nx_i^2 + \beta_1^2\sum_{i=1}^nx_i^2 \\
= \sum_{i=1}^n{y_i^2} - \beta_1^2\sum_{i=1}^nx_i^2$$

Correlation between X and Y is defined as:
$$Cor(X,Y) = \frac{\sum_{i=1}^nx_iy_i}{\sqrt{\sum_{i=1}^nx_i^2}\sqrt{\sum_{i=1}^ny_i^2}}$$
Squaring and substituting in $\beta_1$:
$$ Cor(X,Y)^2 = \frac{(\sum_{i=1}^nx_iy_i)^2}{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i^2} \\
= \frac{\beta_1^2(\sum_{i=1}^nx_i^2)^2}{\sum_{i=1}^nx_i^2\sum_{i=1}^ny_i^2} \\
= \frac{\beta_1^2\sum_{i=1}^nx_i^2}{\sum_{i=1}^ny_i^2} \\
= \frac{\sum_{i=1}^ny_i^2-\sum_{i=1}^ny_i^2+\beta_1^2\sum_{i=1}^nx_i^2}{\sum_{i=1}^ny_i^2} \\
= 1 - \frac{\sum_{i=1}^ny_i^2 - \beta_1^2\sum_{i=1}^nx_i^2}{\sum_{i=1}^ny_i^2} \\
= 1 - \frac{RSS}{TSS} \\
= R^2$$

# Exercise 8

Use the `lm()` function to perform a simple linear regression with mpg as the response and horsepower as the predictor. 

```{r}
library(ISLR)
auto <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Auto.csv", header = TRUE, na.strings = "?")
```

# Exercise 11

Generate a predictor and response:

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)
```

Perform a linear regression without an intercept:

```{r}
lm.fit <- lm(y ~ x + 0)
summary(lm.fit)
```

The high t-value and low p-value indicates a high level of significance so we can reject $H_0$. This is unsurprising given that the response variable is just a transformation of the predictor with some noise added.

Now fit a regression reversing the data:
```{r}
lm.fit1 <- lm(x ~ y + 0)
summary(lm.fit1)
```
The t value and p values are identical, so we can again reject $H_0$, but the standard error is smaller than the first model proportional to the coeffcient estimate. 

The t-statistic for $H_0: \beta = 0$ is $\hat{\beta}/SE(\hat{\beta})$, where $\beta$ is calculated as above in exercise 5 and SE is calculated as:
$$SE(\hat{\beta}) = \sqrt{\frac{\sum_{i=1}^n(y_i-x_i\hat{\beta})^2}{(n-1)\sum_{i'=1}^nx_{i'}^2}}$$
We can show that the t-statistic can be written as follows:
$$(\frac{\hat{\beta}}{SE(\hat{\beta})})^2 = \frac{(\sum_{i=1}^n{x_iy_i})^2}{(\sum_{i=1}^n{x_i^2})^2}\cdot{\frac{(n-1)\sum_{i'=1}^nx_{i'}^2}{\sum_{i=1}^n(y_i-x_i\hat{\beta})^2}} \\
=\frac{(\sum_{i=1}^n{x_iy_i})^2}{(\sum_{i=1}^n{x_i^2})^2}\cdot{\frac{(n-1)\sum_{i'=1}^nx_{i'}^2}{\sum_{i=1}^n{y_i^2} - \beta^2\sum_{i=1}^nx_i^2}} \\
=\frac{(\sum_{i=1}^n{x_iy_i})^2}{(\sum_{i=1}^n{x_i^2})^2}\cdot{\frac{(n-1)\sum_{i'=1}^nx_{i'}^2}{\sum_{i=1}^n{y_i^2} - (\sum_{i=1}^nx_iy_i)^2/\sum_{i=1}^nx_i^2}} \\
={\frac{(\sum_{i=1}^n{x_iy_i})^2(n-1)}{\sum_{i=1}^n{y_i^2}\sum_{i=1}^nx_i^2 - (\sum_{i=1}^nx_iy_i)^2}}\\
\frac{\hat{\beta}}{SE(\hat{\beta})} = \frac{\sqrt(n-1)\sum_{i=1}^nx_iy_i}{\sqrt{\sum_{i=1}^n{y_i^2}\sum_{i=1}^nx_i^2 - (\sum_{i=1}^nx_iy_i)^2}}$$

We can confirm this result numerically:
```{r}
sqrt(length(x) - 1) * sum(x*y) / sqrt(sum(x^2)*sum(y^2) - (sum(x*y))^2)
```
This matches the linear model output shown above. 
As is clear from the derivation shown above, the t-statistic does not depend on the ordering of the variables (i.e. it does not matter if y is regressed on x or vice versa). Hence, the t-statistic is the same for both models. The standard error is different proportionally to the coefficient, so this is consistent with this finding.

We can also show that when the regression is performed with an intercept, the t-statistics are still the same.
```{r}
summary(lm(y~x))
summary(lm(x~y))
```

# Exercise 12

When performing a linear regression without an intercept, the coefficient when regressing x on y and y on x will be the same where the sum of the squared values in x is the same as the sum of the squared values in y. 

An example of where a regression of X onto Y is different to Y onto X:
```{r}
x <- rnorm(100)
y <- rnorm(100)
lm(x ~ y + 0)
lm(y ~ x + 0)
```

An example of where a regression of X onto Y is the same as Y onto X:
```{r}
x <- c(rep(2, 25), rep(0,75))
y <- c(rep(5,4), rep(0,96))

lm(x ~ y + 0)
lm(y ~ x + 0)
```

# Exercise 13

Create a feature X using the `rnorm(100)` command. 
```{r}
set.seed(1)
x <- rnorm(100)
```
Create a vector `eps` from a normal distribution with mean zero and standard deviation of 0.25.
```{r}
eps <- rnorm(100, 0, 0.25)
```
Now generate a vector `y` according to the model $Y = -1 + 0.5X + \epsilon$
```{r}
y <- -1 + 0.5*x + eps
length(y)
```
`y` is of length 100. In the model, $\beta_0$ is -1 and $\beta_1$ is 0.5.
```{r}
plot(x,y)
```
The scatterplot shows a cluster of values for x around 0, the mean, with more sparse points going up to 3 and -3, in line with the parameters set when the vector was created. y takes on values between -2.5 and 0.5, with the majority of points around -1 coinciding with x at zero. The relationship between the two appears linear with a slope of approximately 0.5 (the range of x is about 6 - between 3 and -3 - and the range of y is about 3 - between -2.5 and 0.5). These findings are consistent with the linear model used to generate y. 

Fitting a linear model: 
```{r}
lm.fit <- lm(y ~ x)
summary(lm.fit)
```
The intercept of -1.00942 and coefficient of 0.49973 are quite close to the model used to generate the data. The intercept is within one estimated standard error of the true value, and the coefficient is within just over one SE of the true value. This is reflected in the extremely small p-values.
```{r}
plot(x,y)
abline(lm.fit, col = "blue")
abline( -1, 0.5, col= "red")
legend(x = "bottomright", legend = c("model", "true"), lwd = 3, col = c("blue", "red"))
```
What happens if we add a quadratic term to the model?
```{r}
lm.fit2 <- lm(y ~ x + I(x^2))
summary(lm.fit2)
```
There is no improvement in fit with the addition of a quadratic term - we cannot reject the null hypothesis that the quadratic term's coefficient is non-zero, and furthermore, the increase in the $R^2$ value is minimal, indicating that the model has not been improved. 

Experimenting with different levels of noise in the data:
```{r}
eps <- rnorm(100, 0, 0.1)
y <- -1 + 0.5*x + eps
lm.fit3 <- lm(y ~ x)
summary(lm.fit3)
plot(x,y)
abline(lm.fit3, col = "blue")
abline(-1,0.5,col = "red")
legend(x = "bottomright", legend = c("model", "true"), lwd = 3, col = c("blue", "red"))
```
With less noise, the coefficient and the intercept are both well within one estimated standard error of the true value. This is clear from the plot, where the linear model line and true population line are indistinguishable. 
```{r}
eps <- rnorm(100, 0, 0.5)
y <- -1 + 0.5*x + eps
lm.fit4 <- lm(y ~ x)
summary(lm.fit4)
plot(x,y)
abline(lm.fit4, col = "blue")
abline(-1,0.5,col = "red")
legend(x = "bottomright", legend = c("model", "true"), lwd = 3, col = c("blue", "red"))
```
Although the intercept and coefficients are still close to the true values, the p-value of the intercept is noticeably larger than in the previous two cases. The plot shows a worse fit than the first model, with a clearly different slope and intercept compared to the true function. 

We can see that the confidence intervals are wider in the case with more noise and narrower with less noise. This is consistent with the p-values discussed above. More noise therefore diminishes the power of hypothesis tests. 
```{r}
confint(lm.fit)
confint(lm.fit3)
confint(lm.fit4)
```

# Exercise 14

Setting up some feature vectors:
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```
`y` is the following linear model:
$$Y = 2 + 2X_1 + 0.3X_2 + \epsilon$$
2 and 0.3 are the coefficients. 

The two features are highly correlated with a clear linear relationship shown in the scatter plot. 
```{r}
cor(x1,x2)
plot(x1,x2)
```

```{r}
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
```
```{r}
lm.fit2 <- lm(y ~ x1)
summary(lm.fit2)
```

```{r}
lm.fit3 <- lm(y ~ x2)
summary(lm.fit3)
```

