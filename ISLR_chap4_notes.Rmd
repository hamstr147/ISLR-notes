---
title: "ISLR Chapter 4 - Classification"
output: html_notebook
---

Classification problems are where we have a categorical response. Linear regression is often not suitable for these sort of problems because:

* If we have multiple categories in our response we have to impose an ordering when we construct a numeric encoding for linear regression, even if no natural ordering exists.
* If we want to model probabilities, linear regression can give outputs outside of the 0,1 range.

Note however that the classifications themselves given by linear regression will be the same as LDA). 

# Logistic regression

We can use logistic regression to model the probability of each category in our response variable given the predictor variables. We can then use a decision rule for classification based on the probabilities (e.g. greater than 0.5). 

To constrain outputs to between 0 and 1, we can use the logistic function to model the probabilities:
$$p(X) = \frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}}$$
This implies that the odds are given by:
$$\frac{p(X}{1-p(X)} = e^{\beta_0 + \beta_1x}$$
And so the log odds (logit) are given by:
$$log(\frac{p(X}{1-p(X)}) = \beta_0 + \beta_1x$$
Note that unlike in linear regression, a unit increase in X does not imply an increase in the probability or response by $\beta_1$, because the logistic function is non-linear - the impact depends on the value of X. This linear result holds for the log odds. However, a positive coefficient always implies an increase in the probability with an increase in X and vice versa.  

## Estimating the regression coefficients

We estimate the regression coefficients using the maximum likelihood method. Intuitively, this involves finding estimates that give p(X) as close to the actual observed response for that case. So if we are dealing with credit card default, for example, we want estimates that are as close to 1 for defaults and as close to 0 for non-defaults as possible. Mathematically, we maximise the likelihood function:
$$l(\beta_0,\beta_1) = \prod_{i:y_i=1}p(x_i)\prod_{i':y_{i'}=1}(1-p(x_{i'}))$$
Note that in logistic regression output, the z-statistic fulfils the same role as the t-statistic in the linear regression case, in that it is calculated in the same way as the coefficient over the estimated standard error, and tests the null hypothesis that $\beta_1 = 0$.

## Making predictions
Once we have estimated the coefficients, we can just plug them into the logistic function with a value of X to predict a response. Note that you can use qualitative predictors using the dummy variable approach, in the same way as in the linear regression case. 

## Multiple logistic regression
The simple logistic regression case generalises to cases with multiple predictors, and we estimate the parameters using maximum likelihood in the same way:
$$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X_1 + .. + \beta_pX_p \\
p(X) = \frac{e^{\beta_0 + \beta_1X_1 + .. + \beta_pX_p}}{1+ e^{\beta_0 + \beta_1X_1 + .. + \beta_pX_p}}$$
Note that logistic regression can be used where there are mutliple categories in the response, but they are not covered here. Instead, we can use discriminant analysis. 

# Linear discriminant analysis (LDA)
With LDA, instead of directly modelling probabilities, we model the distribution of the predictors X separately, and then use Bayes' Theorem to calculate Pr(Y = k|X = x). If these distributions are assumed to be normal, the outcome is very similar to linear regression. 

3 main advantages that LDA might have over logistic regression:

* With well-separated clasess, the predictions from LDA are more stable than logistic regression.
* If n is small and the predictors are approximately normally distributed, LDA is again more stable. 
* LDA is a popular method with more than 2 classes in the response. 

## Using Bayes' Theorem for classification
We want to classify an observation into one of $K$ classes, where where $K\geq2$. Let $\pi_k$ represent the overall prior probability that a randomly selected observation belongs to the $k^{th}$ class. Let $f(x)\equiv Pr(X=x|Y=k)$ be the density function of $X$ for an observation that comes from a class $k$. This just means that $f$ is a function that is large if there is a high probability that an observation has $X\approx x$ and is small when this probability is low. Bayes' Theorem states that:
$$Pr(Y=k|X=x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}$$
Abbreviating $Pr(Y=k|X=x)$ to $p_k(x)$, $p_k(x)$ is referred to as the posterior probability that an observation belongs to class $k$ given $X = x$. $\pi_k$ is easy to determine - just take the proportion of your sample that is in the $k^{th}$ class. So all we need to do is find some way of estimating $f_k(x)$, and then we have a method that approximates the Bayes' classifier, which classifies to the class where $p_k(X)$ is largest, and hence gives the lowest possible error rate (assuming the formula is correctly specified).

## LDA for p = 1
LDA essentially involves assuming that observations from each class come from a normal distribution with a class-specific mean and a common variance across all classes - we then plug estimates for these into the Bayes' classifier. 

Let's assume that the density function is Gaussian, so takes the form:
$$f_k(x) = \frac{1}{\sqrt{2\pi\sigma_k}}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)$$
where $\mu_k$ and $\sigma_k^2$ are the mean and variance of the $k^{th}$ class, repsectively. If we also assume that the variances of each class are the same, which we denote by $\sigma^2$, we can plug these values into the formula in the previous section.
$$p_k(x) = \frac{\pi_k\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{1}{2\sigma^2}(x-\mu_l)^2)}$$
Taking logs and rearranging, we can show that this is equivalent to assigning to the class for which the following is greatest:
$$\delta_k(x) = x\cdot{\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(\pi_k)}$$
We still need to estimate the means, variance and prior probabilities - the LDA method uses the following estimates:
$$\hat{\mu}_k=\frac{1}{n}\sum_{i:y_i=k}x_i\\
\hat{\sigma}^2=\frac{1}{n-K}\sum_{k=1}^K\sum_{y_i=k}(x-\hat{\mu}_k)^2$$
where $n$ is the number of training observations and $n_k$ is the number of observations in the $k^{th}$ class. The mean estimate is just the sample average for each class, while the variance estimate is a weighted average of each class's variance. 
If we don't have any knowledge of the prior probabilities for each class, we can just esimtate them:
$$\hat{\pi}_k=n_k/n$$
Hence, LDA assigns an observation to the class for which the following is largest:
$$\hat{\delta}_k(x) = x\cdot{\frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} +log(\hat{\pi_k})}$$
Linear discriminant analysis is 'linear' because the discriminant function is a linear function in x.

## LDA for p > 1
For p > 1, assume $X = (X_1, X-2, ..., Xp)$ is drawn from a multivariate Gaussian distribution with a class-specific vector and a common covariance matrix. To indicate this we write $X ~ N(\mu, \Sigma)$ where $E(X) = \mu$ is the mean of X (a vector with p components) and $Cov(X) = \Sigma$ is the covariance matrix of X. Formally, this is:
$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$
LDA with p > 1 assumes that an observation from the kth class is drawn from a multivariate Gaussian distribution $N(\mu_k, \Sigma)$, where $\mu_k$ is the class-specific mean vector and $\Sigma$ is the covariance matrix common to each class. Plugging in the density function for class k $f_k(X=x)$ into the Bayes classifier and manipulating a bit, we find that the discriminant function is:
$$\delta_k(x) = x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k+log\pi_k$$
LDA assigns an observation to the class where the discriminant function is largest. 

## Performance of classification methods

* Training error rate is likely to be lower than test error rate as overfitting is likely to play a role. The higher the ratio of parameters p to number of observations n, the more likely we are to overfit. 
* If you have unbalanced classes, for instance where one class is quite rare, a null classifier (that always classifies to the more common class) might perform quite well. 

We can create confusion matrices, which cross-tabulates actuals against predictions in a two-by-two table. We want all our results to be on the diagonal. 

Class specific performance may be of more interest than overall error: 

* Sensitivity: Percentage of true observations in the class we're interested in (e.g. debt default, have a disease) - true positives
* Specificity: Percentage of true observations not in that class that are identified - true negatives

Note that LDA might do poorly in such situations because it tries to replicate the Bayesian classifier, which minimises error rate regardless of what class the error happens in. 

Rather than making our decision boundary at p = 0.5, we can have a lower probability, which, while increasing total error, might decrease the error for the class we are interested in - where this is set depends on domain specific knowledge. 

We can use ROC curves (which plot true positive rate against false positive rate) to evaluate performance. A curve that hugs the top left of the plotting space is performing well, while one on the 45 degree line is no better than flippinga coin. We often calculate area under the curve (AUC) to compare performance of different classifiers.

See p.149 for a summary of different terms (false and true positive rate, specificity, sensitivity, precision etc.) used in this context. 

## Quadratic discriminant analysis
QDA works in a similar way to LDA, but assumes that each class has its own covariance matrix - i.e. that observations from class k are drawn from a distribution of the form $N(\mu_k, \Sigma_k)$.

The Bayes classifier classifies an observation $X=x$ to the class for which the following is largest:
$$\delta_k(x)=-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu)-\frac{1}{2}log|\Sigma_k|+log\pi_k\\
= -\frac{1}{2}x^T\Sigma^{-1}_kx+x^T\Sigma^{-1}_k\mu_k-\frac{1}{2}\mu^T_k\Sigma^{-1}_k\mu_k-\frac{1}{2}log|\Sigma_k|+log\pi_k$$
So we need estimates of the class-specific mean vectors, covariance matrices and prior probabilities for QDA to categorise X=x to a particular class. 

# Lab
Use the Smarket dataset from the ISLR library:
```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
```
USe `cor` to give a correlation matrix. Omit 'Direction' variable as it is qualitative:
```{r}
cor(Smarket[,-9])
```
There is essentially no correlation between lagged and current returns. The only discernable correlation here is Volume and Year - trading volumes increase each year:
```{r}
plot(Smarket$Volume)
```

## Logistic regression
Using logistic regression to predict Direction:
```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = "binomial")
summary(glm.fit)
```
Use `coef` to get coefficients, or you can access parts of the model using `summary`:
```{r}
coef(glm.fit)
summary(glm.fit)$coefficients
summary(glm.fit)$coefficients[,4]
```
Check encoding using `contrasts`- Up is encoded as 1:
```{r}
contrasts(Smarket$Direction)
```
Using `predict` with `type = response` gives probabilities that the market will go Up (as Up is encoded as 1). Get the first 10 probabilities for the training data:
```{r}
glm.probs <- predict(glm.fit, type = "response")
glm.probs[1:10]
```



